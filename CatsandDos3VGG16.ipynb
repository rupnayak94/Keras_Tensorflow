{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications import VGG16\n",
    "from keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"D:/Docs/Education/Data Science ProDegree Imarticus/Python projects/Analytics Vidya and Kaggle/CatsDogs/train\"\n",
    "valid_dir=\"D:/Docs/Education/Data Science ProDegree Imarticus/Python projects/Analytics Vidya and Kaggle/CatsDogs/valid\"\n",
    "test_dir=\"D:/Docs/Education/Data Science ProDegree Imarticus/Python projects/Analytics Vidya and Kaggle/CatsDogs/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generator\n",
    "\n",
    "Used for Data Augmentation technique for generating Multiple images from a Single Image.\n",
    "Images in this generator are generated by:\n",
    "1. Rotate the image randomly upto 40degs\n",
    "2. Zooms, Witdth and Height change\n",
    "3. Horizontal flips etc\n",
    "\n",
    "We will use separate Valid generator so keep valid data as intact without much modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen=ImageDataGenerator(rescale=1./255,rotation_range=40, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                 shear_range=0.2, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control batch and resolution of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=150\n",
    "batch=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and valid generator data flow from Disk\n",
    "\n",
    "1. The colour mode is controled by \"color_mode\" argument, \"grayscale\" and \"rgb\",\n",
    "2. Target resolution in controlled by \"target_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator=train_data_gen.flow_from_directory(train_dir, target_size=(res,res), \n",
    "                                        batch_size=batch, class_mode=\"binary\", color_mode=\"rgb\")\n",
    "valid_generator=valid_data_gen.flow_from_directory(valid_dir, target_size=(res,res), \n",
    "                                        batch_size=batch, class_mode=\"binary\", color_mode=\"rgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "For test data we will not use any generator since the class is not predefined. \n",
    "For generators class need to be predifinited and should be stored in separate folders before generators can be used. Since the test data's class are not defined at all, we will use Keras load_img function to load images of specific resolution unmodified (no data augmentation)\n",
    "\n",
    "load_img read a single image at a time so we will provide each image as a path to the load_img function\n",
    "\n",
    "this same function can also be achieved by openCv (CV2)\n",
    "\n",
    "The colour mode is controled by \"color_mode\" argument, \"grayscale\" and \"rgb\" etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst=[]\n",
    "for img_path in os.listdir(test_dir):\n",
    "    imgno=img_path.split(\".\")[0]\n",
    "    path=os.path.join(test_dir,img_path)\n",
    "    img_pip=load_img(path, color_mode=\"rgb\", target_size=(res,res))\n",
    "    img_ar=img_to_array(img_pip)\n",
    "    tst.append([np.array(img_ar), np.array(imgno)])\n",
    "    \n",
    "    \n",
    "TestX=np.array([i[0] for i in tst])\n",
    "ids=np.array([i[1] for i in tst])\n",
    "\n",
    "TestX=TestX/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the VGG16\n",
    "\n",
    "VGG16 has been trainned in million of animal images which learning can be used for better prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base=VGG16(weights=\"imagenet\", include_top=False, input_shape=(150,150,3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model builing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=models.Sequential()\n",
    "model2.add(conv_base)\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(256, activation=\"relu\"))\n",
    "model2.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStop=EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1, mode=\"auto\", \n",
    "                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable=False\n",
    "Setting Trainable to False sets locks the models layers weights to be updated thus helps in retaining on that the model has learned from the prvious setup.\n",
    "\n",
    "We will only connect it with dense layers post flattening or converting to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechecking the dimension \n",
    "after setting the VGG16 layers to False to check the number of traininable and non trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 31,527,041\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rupanjan/anaconda3/envs/tfgpu/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "model2.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model2.fit_generator(train_generator, steps_per_epoch=350, epochs=20, \n",
    "                          validation_data=valid_generator, validation_steps=50, callbacks=[EarlyStop],\n",
    "                          workers=300, max_queue_size=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tunning\n",
    "\n",
    "Fine tunning of the VGG16 Convolution layer can be done by setting one of the layers as trainable thus while most of the weigths remain same from what the model has learned from its previous experience of million of images, it can be slightly modifiled to suite the new data which are training on.\n",
    "\n",
    "The method by which is is done is\n",
    "1. We set one of the 5 (1/5) layers of the trainable while rest of the (4/5) layers are locked.\n",
    "2. Thus in the 4/5 or the 4 of the 5 layers the model retains whats it has previously learned.\n",
    "3. The last or the 5th layers updates its weights based on what it has learned from the training data generated from the image generators at a very low learning rate thus fines tunes the model based on the current data.\n",
    "4. It updates the weight based on the new data thus learns patterns from the new data and thus fines tunes the model to the current data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The if statement \"if set_trainable:\" makes sure the whole block_5 or the whole block gets updated instead of just one convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable=True\n",
    "set_trainable=False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name==\"block5_conv1\":\n",
    "        set_trainable=True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 False\n",
      "block1_conv1 False\n",
      "block1_conv2 False\n",
      "block1_pool False\n",
      "block2_conv1 False\n",
      "block2_conv2 False\n",
      "block2_pool False\n",
      "block3_conv1 False\n",
      "block3_conv2 False\n",
      "block3_conv3 False\n",
      "block3_pool False\n",
      "block4_conv1 False\n",
      "block4_conv2 False\n",
      "block4_conv3 False\n",
      "block4_pool False\n",
      "block5_conv1 True\n",
      "block5_conv2 True\n",
      "block5_conv3 True\n",
      "block5_pool True\n"
     ]
    }
   ],
   "source": [
    "for l in conv_base.layers:\n",
    "    print(l.name, l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=optimizers.RMSprop(lr=1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "EarlyStop=EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1, mode=\"auto\", \n",
    "                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1200/1200 [==============================] - 578s 482ms/step - loss: 0.2702 - accuracy: 0.8820 - val_loss: 0.1016 - val_accuracy: 0.9399\n",
      "Epoch 2/20\n",
      "1200/1200 [==============================] - 548s 456ms/step - loss: 0.1805 - accuracy: 0.9252 - val_loss: 0.1223 - val_accuracy: 0.9607\n",
      "Epoch 3/20\n",
      "1200/1200 [==============================] - 546s 455ms/step - loss: 0.1535 - accuracy: 0.9370 - val_loss: 0.1226 - val_accuracy: 0.9636\n",
      "Epoch 4/20\n",
      "1200/1200 [==============================] - 546s 455ms/step - loss: 0.1340 - accuracy: 0.9451 - val_loss: 0.0253 - val_accuracy: 0.9671\n",
      "Epoch 5/20\n",
      "1200/1200 [==============================] - 551s 459ms/step - loss: 0.1194 - accuracy: 0.9518 - val_loss: 0.0386 - val_accuracy: 0.9623\n",
      "Epoch 6/20\n",
      "1200/1200 [==============================] - 544s 454ms/step - loss: 0.1095 - accuracy: 0.9555 - val_loss: 0.0448 - val_accuracy: 0.9648\n",
      "Epoch 7/20\n",
      "1200/1200 [==============================] - 544s 453ms/step - loss: 0.0988 - accuracy: 0.9609 - val_loss: 0.0223 - val_accuracy: 0.9645\n",
      "Epoch 8/20\n",
      "1200/1200 [==============================] - 544s 454ms/step - loss: 0.0897 - accuracy: 0.9640 - val_loss: 0.1601 - val_accuracy: 0.9691\n",
      "Epoch 9/20\n",
      "1200/1200 [==============================] - 546s 455ms/step - loss: 0.0837 - accuracy: 0.9676 - val_loss: 0.0165 - val_accuracy: 0.9738\n",
      "Epoch 10/20\n",
      "1200/1200 [==============================] - 553s 461ms/step - loss: 0.0766 - accuracy: 0.9699 - val_loss: 0.0607 - val_accuracy: 0.9725\n",
      "Epoch 11/20\n",
      "1200/1200 [==============================] - 546s 455ms/step - loss: 0.0709 - accuracy: 0.9724 - val_loss: 0.1002 - val_accuracy: 0.9616\n",
      "Epoch 12/20\n",
      "1200/1200 [==============================] - 547s 456ms/step - loss: 0.0639 - accuracy: 0.9754 - val_loss: 0.1240 - val_accuracy: 0.9677\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist=model2.fit_generator(train_generator, steps_per_epoch=1200, epochs=20, \n",
    "                          validation_data=valid_generator, validation_steps=50, callbacks=[EarlyStop],\n",
    "                          workers=300, max_queue_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting of Loss and accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_acc():\n",
    "    hist_dict=hist.history\n",
    "    \n",
    "    train_acc=hist_dict[\"accuracy\"]\n",
    "    valid_acc=hist_dict[\"val_accuracy\"]\n",
    "    train_loss=hist_dict[\"loss\"]\n",
    "    valid_loss=hist_dict[\"val_loss\"]\n",
    "    \n",
    "    epoch=range(1,len(train_acc)+1)\n",
    "    \n",
    "    #ploting loss and accuracy\n",
    "    #plot loss\n",
    "    plt.plot(epoch, train_loss, \"bo\", label=\"Train loss\")\n",
    "    plt.plot(epoch, valid_loss, \"r\", label=\"Valid loss\")\n",
    "    \n",
    "    #plot accuracy\n",
    "    plt.plot(epoch, train_acc, \"go\", label=\"Train Acc\")\n",
    "    plt.plot(epoch, valid_acc, \"black\", label=\"Valid Acc\")\n",
    "    \n",
    "    plt.title(\"Acc and loss\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "plot_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=model2.predict(TestX).ravel()\n",
    "submission=pd.DataFrame({\"id\": ids, \"label\": predict})\n",
    "submission.to_csv(\"sub01_cov2d_32641282561024512_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowGpu",
   "language": "python",
   "name": "tensorflowgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
